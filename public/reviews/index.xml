<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reviews | what dat data do?</title>
    <link>/reviews/</link>
      <atom:link href="/reviews/index.xml" rel="self" type="application/rss+xml" />
    <description>Reviews</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>/img/cover_photo.jpg</url>
      <title>Reviews</title>
      <link>/reviews/</link>
    </image>
    
    <item>
      <title>Agnostic Notes on Regression Adjustments to Experimental data: Reexamining Freedman’s Critique</title>
      <link>/reviews/freedmancritique/freedmancritique/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/reviews/freedmancritique/freedmancritique/</guid>
      <description>


&lt;p&gt;In the usual set up of estimating average causal effect in a randomized experiment, Freedman criticized using OLS coefficient of treatment as an estimate when regressing observed outcome on treatment and covariates. To reiterate, based on Neyman’s model, we assume there are n subjects (finite population of interest), assignment (Z) is the only source of randomness, covariates (X), potential outcomes Y(1) and Y(0) are fixed, observed outcomes (Y) are random because they depend on assignment.&lt;/p&gt;
&lt;p&gt;Lin overall agrees that just because you have random assignment does not mean you will automatically get necessarily better estimates if you regress with covariates but shows that including a full interaction term will make the estimate at least as good as the two popular ones, ATEunadf (simple difference in means, not considering covariates) and ATEadf (regressing outcome on covariates all at once). In short, Lin’s proposal is to add the interaction term: under the same regularity conditions, OLS adjustment will not ‘worsen’ efficiency (as contended by Freedman), Hubert White error estimate is still consistent and asymptotically normal (regardless of whether the interaction term is included) and finally the bias of ATEinteract tends to zero with large enough sample sizes. He creates an intuitive explanation by drawing an analogy between survey sampling and randomised experiments. To summarise, Lin’s main point is that little needs to be changed in the conventional finite population inference to address freedman’s criticism.&lt;/p&gt;
&lt;p&gt;Lin’s argument as to why we should include covariates is by analogising the estimation of a population mean. Intuitively, suppose you want to predict the mean of a population Y, but you only have a sample, say, Ys. But you have the whole population of some covariate, say X, where X is something that may or may not have correlation with Y. Mean(YS) will be unbiased but will ignore this correlation. A ‘better’ estimate would incorporate this association (or lack thereof): estimating the population mean by including the sample mean, but also the covariate X, whose association with Y we can estimate from the sample.&lt;/p&gt;
&lt;p&gt;So the respective argument for randomized experiment is that we have data (covariates) for the entire population (N) so including it to estimate ATE will not worsen the estimate. If there is no association between Y and X, the estimate will just boil down to ATEadj. If there is, ATEinteract will have lower variance because it ‘adjusts’ the estimate by giving more importance to the group with fewer units, because this group will have a mean estimate that more biased than the group with more units. Note that if the groups have equal units, then even under ATEadj, the ‘overestimation’ and ‘underestimation’ will cancel out, leading to the same estimate as ATEinteract.&lt;/p&gt;
&lt;p&gt;This naturally explains Lin’s argument for separate regression because for the treatment group, we consider total population to be n and to get average treatment effect we consider it to be a sample mean of n1 units - for control we consider a sample mean of n0 units. We estimate the means of Y(1) and Y(0) separately, and then estimate ATE. If the covariates have exact opposite correlation, ATEinteract reduces to ATEunadj - (the differences cancel out), if not, ATEinteract has lower variance. Note that ATEunadj is a specific case of ATEinteract, if we consider ATEinteract to be the difference of two ‘fixed slope’ regression estimators, in which case ATEunadj places a value of zero in the scaling factor.&lt;/p&gt;
&lt;p&gt;Lin further argues and demonstrates that Freedman’s contention that the OLS error estimate is inconsistent can be addressed by using the Hubert White error instead: This estimate is also agnostic, meaning that it shows nice asymptotic properties even if the regression model is incorrect. Intuitively, ATEinteract has standard error which includes the variance in Y(1) and Y(0) unexplained by the covariates but ‘inversely’ scaled. This also leads to a discussion of an intrinsic parallel between Lin’s estimator and a weighted OLS of Y on X and Z with these ‘inverse’ scales as weight.&lt;/p&gt;
&lt;p&gt;The difference between variances of ATEinteract and ATEadj can be thought of as the &lt;strong&gt;‘paired variance’ - ‘how better the OLS coefficients are when regression is done separately’&lt;/strong&gt; - whereas the difference between variances of ATEinteract and ATEunadj can be thought of as &lt;strong&gt;‘pooled variance’ - ‘how much of Y the covariates are able to explain’.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lastly, it is also notable that in case the covariate is categorical, ATEinteract gives the same estimate as ATEstra, that is, post-stratification ATE.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables: An Econometrician’s Perspective</title>
      <link>/reviews/iv/iv-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/reviews/iv/iv-analysis/</guid>
      <description>


&lt;p&gt;Econometricians had begun to develop the concept of instrumental variables, long before statisticians, for drawing causal inferences without the assumption of random assignment. They were interested in studying the effect of economic policies on human behavior. From the very beginning, they acknowledged that the level of treatment is often actively influenced by economic agents (e.g.. households). Therefore, the units in question are not only different in terms of treatment received, but the active decision to seek out that treatment. This sort of a setting refers to an endogenous treatment. This of course gives rise to selection bias, which is completely Unobservable. Instrumental variables were not given attention by the statistics community because of how economic-theory-centric they were.&lt;/p&gt;
&lt;p&gt;Statistics literature traditionally looked at assignment as randomised: possibly because they looked at inanimate or passive objects like plots of land. The Fisher and Neyman concept of potential outcomes of randomised experiments was extended to general observational studies by scholars like Rubin, Rosennbaum, by using covariates, propensity scores etc. Unconfoundedness conditional on covariates, is analogous to selection-on-observables or exogeneity in econometric literature. In contrast, economic literature studied economic agents that are active agents that make their own decisions subject to constraints of resources and also personal judgement. That these agents will behave ‘rationally’ to maximise utility is a traditional starting point (often not very practical). As an example, if we as statisticians were interested in observing the difference in productivity of fishing and hunting, we do a simple ATE. Maybe we condition on characteristics like income. However, economists start by assuming that each individual will choose the occupation the maximises their productivity, and then get bounds for ATE. This has some faults: how can individual always know which occupation will serve them better? Also, they might find other factors (aside from productivity) that they might take into consideration like family history or something even irrational. The link between the two approaches is that individuals with similar covariates are still comparable. Instrumental variables are thus forces that influence agents to choose different treatments without changing the possible potential outcomes. This assumption is a bit controversial because it is fundamentally untestable.&lt;br /&gt;
Imbens explains the concept of simultaneous equations of supply and demand in the context of endogeneity with an example of a fish market with small supply and a large demand. If we wanted to estimate the effect of a new tax on fish sold, the problem that an investigator would have is that we only observe one potential outcome - nobody yet pays the tax. So we never observe Y(with tax) for any of the units. But by using observed data and general regression (or perhaps some other method), we can come up with an estimate of how much demand will fall. But this doesn’t take into account that prices are not independent of quantities traded, that is, violation of unconfoundedness. Maybe on a particular day there was less fish yield, or conversely maybe buyers bought in bulk. Therefore we would need to adjust for how both the buyers and sellers react to the change in price in order to estimate the effect of tax increase, through the help of demand and supply equations. Instrumental variables come into the picture by identifying factors that affect supply but not demand (like ocean conditions) and vice versa. A more modern example that Imbens provides is one of a randomised experiment with non compliance, interrelated with encouragement design. If we consider encouragement (yes/no) and acceptance of encouragement binary, we have four potential situations with respect to the different arrangements. Encouragement assignment obviously need to be correlated with treatment for any substantial inferences. (weak IVs arise when this correlation is low). We assume that encouragement assignment is independent of potential outcomes. (we can always assume that encouragement assignment is unconfounded conditional on some covariates X). Under both conditions, we can talk about the intention to treat, under the assumption of exclusion restriction, that encouragement assignment does not affect potential outcome of treatments (which is again, controversial), of monotonicity which essentially means that we rule out the case of defiers or those who end up doing the opposite of encouragement: which means that the probability that a patient receives a vaccine, say for example, will not lower just because their doctor received a letter of encouragement.&lt;/p&gt;
&lt;p&gt;So we can finally understand what complier effect means: out of all the different cases, how did the average outcome change when response to encouragement was compliant with encouragement? Although we can’t identify the complier population directly, we can identify those who don’t fall into it. Hence, we can consider this a ‘second-best’ analysis setting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables: An Econometrician’s Perspective</title>
      <link>/reviews/iv/iv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/reviews/iv/iv/</guid>
      <description>


&lt;p&gt;Econometricians had begun to develop the concept of instrumental variables, long before statisticians, for drawing causal inferences without the assumption of random assignment. They were interested in studying the effect of economic policies on human behavior. From the very beginning, they acknowledged that the level of treatment is often actively influenced by economic agents (e.g.. households). Therefore, the units in question are not only different in terms of treatment received, but the active decision to seek out that treatment. This sort of a setting refers to an endogenous treatment. This of course gives rise to selection bias, which is completely Unobservable. Instrumental variables were not given attention by the statistics community because of how economic-theory-centric they were.&lt;/p&gt;
&lt;p&gt;Statistics literature traditionally looked at assignment as randomised: possibly because they looked at inanimate or passive objects like plots of land. The Fisher and Neyman concept of potential outcomes of randomised experiments was extended to general observational studies by scholars like Rubin, Rosennbaum, by using covariates, propensity scores etc. Unconfoundedness conditional on covariates, is analogous to selection-on-observables or exogeneity in econometric literature. In contrast, economic literature studied economic agents that are active agents that make their own decisions subject to constraints of resources and also personal judgement. That these agents will behave ‘rationally’ to maximise utility is a traditional starting point (often not very practical). As an example, if we as statisticians were interested in observing the difference in productivity of fishing and hunting, we do a simple ATE. Maybe we condition on characteristics like income. However, economists start by assuming that each individual will choose the occupation the maximises their productivity, and then get bounds for ATE. This has some faults: how can individual always know which occupation will serve them better? Also, they might find other factors (aside from productivity) that they might take into consideration like family history or something even irrational. The link between the two approaches is that individuals with similar covariates are still comparable. Instrumental variables are thus forces that influence agents to choose different treatments without changing the possible potential outcomes. This assumption is a bit controversial because it is fundamentally untestable.&lt;br /&gt;
Imbens explains the concept of simultaneous equations of supply and demand in the context of endogeneity with an example of a fish market with small supply and a large demand. If we wanted to estimate the effect of a new tax on fish sold, the problem that an investigator would have is that we only observe one potential outcome - nobody yet pays the tax. So we never observe Y(with tax) for any of the units. But by using observed data and general regression (or perhaps some other method), we can come up with an estimate of how much demand will fall. But this doesn’t take into account that prices are not independent of quantities traded, that is, violation of unconfoundedness. Maybe on a particular day there was less fish yield, or conversely maybe buyers bought in bulk. Therefore we would need to adjust for how both the buyers and sellers react to the change in price in order to estimate the effect of tax increase, through the help of demand and supply equations. Instrumental variables come into the picture by identifying factors that affect supply but not demand (like ocean conditions) and vice versa. A more modern example that Imbens provides is one of a randomised experiment with non compliance, interrelated with encouragement design. If we consider encouragement (yes/no) and acceptance of encouragement binary, we have four potential situations with respect to the different arrangements. Encouragement assignment obviously need to be correlated with treatment for any substantial inferences. (weak IVs arise when this correlation is low). We assume that encouragement assignment is independent of potential outcomes. (we can always assume that encouragement assignment is unconfounded conditional on some covariates X). Under both conditions, we can talk about the intention to treat, under the assumption of exclusion restriction, that encouragement assignment does not affect potential outcome of treatments (which is again, controversial), of monotonicity which essentially means that we rule out the case of defiers or those who end up doing the opposite of encouragement: which means that the probability that a patient receives a vaccine, say for example, will not lower just because their doctor received a letter of encouragement.&lt;/p&gt;
&lt;p&gt;So we can finally understand what complier effect means: out of all the different cases, how did the average outcome change when response to encouragement was compliant with encouragement? Although we can’t identify the complier population directly, we can identify those who don’t fall into it. Hence, we can consider this a ‘second-best’ analysis setting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sex Bias in Graduate Admissions: Data from Berkeley</title>
      <link>/reviews/simpsonparadox/simpsonparadox/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/reviews/simpsonparadox/simpsonparadox/</guid>
      <description>


&lt;p&gt;The purpose of this paper is to demonstrate how potentially incorrect conclusions about trends can be made from the available data in certain cases where either latent variables aren’t accounted for, or if certain assumptions of the approach are actually false. In particular, the purpose is to investigate if the claim made that UC Berkeley Graduate Schools (in the fall of 1973, with approximately 12763 applicants) were biased against admitting women is valid. The morality, legality or social ramifications or the consequences that may lead to these biases is not in the statistical scope of the paper, nor does the paper claim through its conclusions that ‘bias’ or ‘discrimination’ can be fully understood through mere admission rates. Thus, ‘bias’ is defined as a pattern of association between admission decisions and sexes of applicants. Any approach to understand the sex trends of admission is based on the &lt;strong&gt;primary assumption&lt;/strong&gt; that for any given discipline, men and women do not differ in terms of any attribute deemed germane to their acceptance; if not, any discussion of ‘sex bias’ is not meaningful.&lt;/p&gt;
&lt;p&gt;The initial approach was to pool all the data for men and women, with the &lt;strong&gt;second main assumption&lt;/strong&gt; that for each discipline, men and women are equally likely to apply. A contingency table of sex-wise total data led to chi square of 110.8 with 1 degree of freedom and a p-value of almost zero, which showed evidence of clear bias against women (There were ~277 fewer women than expected). (Expected admitted women = Total Admitted* Total Women that applied/Total Applied) With this conclusion, relevant departments (those that women actually applied to or which rejected women) were investigated, since each department made autonomous admission decisions. But on that granular level, most departments seem to be fair with their admission process, with 26 fewer admitted women in combined 4 departments, but also 64 fewer admitted men in a total of 6 departments. In light of these discrepancies, the validity of the second assumption was called into question: that there is no relation between sex of applicant and the discipline applied for. It was found that the proportion of women applicants tended to be higher in departments that are ‘generally harder’ to get into: i.e that had a lower admission rate. (admission rate = total accepted/total applied). So the next approach was to investigate each department individually, therefore a total of 85 independent experiments, each with their own chi square statistic. Combining these 85 statistics, according to Fisher statistic (which tests one-sided - that is alternate hyp: no bias or bias for women) and E.Scott’s method, yielded that there was &lt;strong&gt;bias against men&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At this point there are two reasons why we doubt the conclusion of first approach: no individual department has disparate admission rate, and even when we aggregate the test statistics for each department, we get found bias against men. The reason why the sex ratios among departments is not random is because not all departments are equally easy to get into: a 2x101 contingency table of admitted vs denied applicants yielded a chi-square statistic of 2195 with a near-zero p-value. Same was corroborated through a weighted correlation test. So two key points here are that men and women have tendencies to apply for different disciplines, for example, only 2% of applicants to mechanical engineering were women, but around 2/3rds of applicants to English were women, and the ones that women applied more to had a lower admittance rate. The next approach was to consider, one by one, the individual departments. But there were a few problems with this approach: how some departments are small so that we can’t apply approximation techniques and how when conducting 85 indivudal experiments, the chance of observing an unusual sex ratio in any one of them by chance alone (Type I error) increases. So a final approach (‘Pooling’) was introduced, which only held the first assumption and accounted for the probability of being accepted in a department and the number of women who applied in that department, to calculate the expected number of women for each department. The expected number for each department were added to get total expected admitted women. Total expected women was actually 60 fewer than observed. Using this method, data from 1969 to 1972 was also observed, indicating an absence of bias. (And if any bias, at all, it was in favour of women). The paper also discusses some perils of pooling data, such as extreme bias in different departments cancelling out. To summarise, this paper discusses a classic case of Simspns’s paradox and found that the conclusions made from aggregated data, which doesn’t take into account non-random sex ratios among departments, are negated by more robust approaches: which found no discrimination against female applicants. According to the paper, women applied more to harder disciplines potentially because of STEM-related entrance requirements, and STEM is a field that women traditionally have been dissuaded from. The ‘job market’ is also tougher for women so they tend to gravitate less towards disciplines that are more job-oriented (and thus ‘easier’ to get into) Women are directed by their education and socialization toward fields that are more crowded, less productive of degrees, and less well funded and that frequently offer poorer professional employment opportunities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The central role of the propensity score in observational studies for causal effects: summary</title>
      <link>/reviews/propensityscores/propensityscores/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/reviews/propensityscores/propensityscores/</guid>
      <description>


&lt;p&gt;Observational studies create barriers for causal studies because of several reasons but primarily, selection bias - a lack of randomisation implies that the potential outcomes are not independent of treatment assignment, and also that units exposed to treatment could (and usually do) differ fundamentally from the control units.&lt;/p&gt;
&lt;p&gt;The paper defines a balancing score as any function of the covariates (which are observed pretreatment or simply: covariates), such that the conditional distribution of x given this score is the same for treated and control units. X (the covariates) itself is balancing, but the objective is to find a many-one balancing score or coarsest to bring about dimension reduction.&lt;br /&gt;
Now, the propensity score &lt;span class=&#34;math inline&#34;&gt;\(e(x) = Pr(Z = 1/ X = x))\)&lt;/span&gt; for non randomised studies does not have a specific form, as opposed to in randomised studies where this probability is known. However, it’s possible to estimate the score from the data (Using for example a generalised linear model like logit). Moreover, propensity score is a balancing score - in fact, it is the ‘finest’ balancing score - it can be represented as a function of any other balancing score.&lt;/p&gt;
&lt;p&gt;If the treatment assignment is ‘strongly ignorable’ - that is, conditional on X (covariates), potential outcomes are independent of assignment, then it is true that conditional on the propensity score, potential outcomes are independent of assignment.&lt;/p&gt;
&lt;p&gt;Conditioning on propensity score gives us an unbiased estimate. Units with the same value of propensity score but different treatments can act as controls for each other; the expected difference in their response equals the average treatment effect. The applications of propensity score extends to pair matching, stratification (discretizing propensity scores) and covariance adjustment.&lt;/p&gt;
&lt;p&gt;The paper discussed three main applications of propensity scores: Pair matching: This means having pairs of treated and control units in our data. First, randomly sample a propensity score, and then get a treatment and control unit with the matching propensity score value. Paired matching given propensity score has lower variance: we saw the exact formulas in lecture. Covariance adjustment: With X as the propensity score, within each strata of propensity score we can do a linear regression: this is the same as OLS with interaction, or Lin’s estimator.&lt;/p&gt;
&lt;p&gt;Stratification: Conditional on propensity score, we can estimate ATE like we would in a stratified experiment. In stratification, using propensity score and not actual covariates is useful because in some cases, stratifying on all covariates can create many more groups which could lead to a situation where within a strata there are no treated or control groups. If &lt;span class=&#34;math inline&#34;&gt;\(e(X)\)&lt;/span&gt; is continuous, we can discretise the probabilities. For most observational studies, it was found that the optimal number for discretization was five.&lt;/p&gt;
&lt;p&gt;The rationale for the use of propensity score in observational studies were several, including: Conditioning on the propensity score makes pair matching for intuitive, and makes the study easier to understand for people with limited statistical background Model-based adjustment become more robust even if ‘true’ model of regressing outcomes on propensity score is incorrect.&lt;/p&gt;
&lt;p&gt;If you have large control group reservoir: say if you are interested in the health of workers working in a Nuclear plant, then it easier to stratify on propensity score, and then sample from the control group, then to do random subgrouping. Of course it is possible to have residual bias in the propensity score: but this itself implies that the estimate for ATE is also biased.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To summarise:&lt;/em&gt; propensity score can be understood as how much ‘importance’ the treatment mechanism is giving to treatment outcome: it signifies the probability that we will observe Y(1), not Y(0). Fitting a model on E(Z/X) can give us unbiased estimates of propensity, and under strong ignorability, there are various ways to estimate the average causal effect in an otherwise non randomised observational study, through pair matching, stratification and covariance adjustment.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

[{"authors":["admin"],"categories":null,"content":"hello!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"hello!","tags":null,"title":"Malvika Rajeev","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6510600d074cfb401da1ee9d796b6e41","permalink":"/post/linguistic-survey-data-can-we-detect-regionalities/lab2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/linguistic-survey-data-can-we-detect-regionalities/lab2/","section":"post","summary":"","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" This data analysis was undertaken as a part of the Cal’s Stat 215A.\n##Please set path before running: path to where the main data is ##libraries: library(kableExtra) library(dbscan) library(factoextra) library(fastcluster) library(FactoMineR) library(NbClust) library(tidyverse) library(magrittr) library(cluster) library(cowplot) library(NbClust) library(clValid) library(ggfortify) library(clustree) library(dendextend) library(factoextra) library(FactoMineR) library(corrplot) library(GGally) library(knitr) library(kableExtra) library(gplots) library(wesanderson) library(ggplot2) library(tidyverse) library(maps) library(crosstalk) library(readr) library(gridExtra) library(leaflet) library(MASS) library(Rtsne) library(irlba) state_df \u0026lt;- map_data(\u0026quot;state\u0026quot;) my_map_theme \u0026lt;- theme_void() # load the data ling_data \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/malvikarajeev/linguisticSurvey/master/lingData.txt\u0026quot;, header = T) ling_location \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/malvikarajeev/linguisticSurvey/master/lingLocation.txt\u0026quot;, header = T) # question_data contains three objects: quest.mat, quest.use, all.ans library(repmis) source_data(\u0026quot;https://github.com/malvikarajeev/linguisticSurvey/blob/master/question_data.RData?raw=true\u0026quot;) ## [1] \u0026quot;quest.mat\u0026quot; \u0026quot;quest.use\u0026quot; \u0026quot;all.ans\u0026quot; answers \u0026lt;- all.ans[50:122] ####USING PACKAGE ZIPCODE library(zipcode) data(\u0026quot;zipcode\u0026quot;) ###changing ZIPs to add a zero zip \u0026lt;- ling_data$ZIP zip \u0026lt;- as.character(zip) for(i in 1:length(zip)) { if(as.numeric(zip[i]) \u0026lt; 10000){ zip[i] \u0026lt;- paste0(\u0026quot;0\u0026quot;, zip[i]) } } ling_data$ZIP \u0026lt;- zip t2 \u0026lt;- merge(ling_data,zipcode, by.x = \u0026#39;ZIP\u0026#39;, by.y = \u0026#39;zip\u0026#39;) t2 \u0026lt;- t2[, -c(2:4, 72, 73)] names(t2)[69:72] \u0026lt;- c(\u0026quot;CITY\u0026quot;, \u0026quot;STATE\u0026quot;, \u0026quot;lat\u0026quot;, \u0026quot;long\u0026quot;) ling_data \u0026lt;- t2 l \u0026lt;- length(all.ans) structure \u0026lt;- matrix(numeric(l*2), l,2) for (i in 1:l){ temp \u0026lt;- all.ans[[i]] structure[i,1] \u0026lt;- temp$qnum[1] structure[i,2] \u0026lt;- length(temp$ans.let) } structure \u0026lt;- as.data.frame(structure) names(structure) \u0026lt;- c(\u0026#39;ques.num\u0026#39;, \u0026#39;number_choices\u0026#39;) #so one person has 73 responses. #for every response, structure$number_choices tells us which response it is. ##missing: 112, 113, 114, 116, 122 struc \u0026lt;- structure[50:122,] struc \u0026lt;- struc[-c(63:65,67,73),] struc \u0026lt;- struc[-59,] #source(\u0026quot;/Users/malvikarajeev/Desktop/stat215/stat-215-a/lab2/R/clean.R\u0026quot;) dummy \u0026lt;- ling_data[,-c(1, 69:72)] names(dummy) \u0026lt;- struc$ques.num answer_no \u0026lt;- struc$number_choices N \u0026lt;- nrow(ling_data) l \u0026lt;- as.data.frame(matrix(numeric(N), N)) for (i in 1:length(names(dummy))){ df \u0026lt;- dummy[i] names(df) \u0026lt;- \u0026quot;answers\u0026quot; df$recode \u0026lt;- list(rep(0, answer_no[i])) df$recode \u0026lt;- Map(function(x,y) `[\u0026lt;-`(x,y,1), x = df$recode, y = df$answers) temp \u0026lt;- data.frame(matrix(unlist(df$recode), nrow=length(df$recode), byrow=T)) l \u0026lt;- c(l, temp) } l \u0026lt;- data.frame(l) l \u0026lt;- l[,-1] #binary \u0026lt;- read.csv(\u0026quot;~/Desktop/stat215/stat-215-a/lab2/data/binary.csv\u0026quot;) #binary \u0026lt;- binary[,-1] ##serial numbers binary \u0026lt;- l ######################################################## ##fix the column names using structure ##creating a column on vector names create_names \u0026lt;- function(x) { return(lapply(x$number_choices, function(x) {seq(1:x)})) } names_col \u0026lt;- create_names(struc) names_ans \u0026lt;- unlist(sapply(1:67, function(x) {paste(struc$ques.num[[x]], names_col[[x]], sep = \u0026quot;_\u0026quot;)})) names(binary) \u0026lt;- names_ans ###################################################### binary$lat \u0026lt;- ling_data$lat binary$long \u0026lt;- ling_data$long binary$id \u0026lt;- ling_data$ID binary$city \u0026lt;- ling_data$CITY binary$state \u0026lt;- ling_data$STATE ##keeping first three zips of dataframe binary$zip \u0026lt;- substr(as.character(ling_data$ZIP),1,nchar(ling_data$ZIP) - 2) ##clear out indivudals who didnt answer all the questions binary \u0026lt;- binary[rowSums(binary[,1:468]) == 67,] ###########################BY ZIP ##group by . have to remove: state, zip, city, lat, long temp \u0026lt;- binary[, -(469:472)] by_zip \u0026lt;- temp %\u0026gt;% group_by(zip) %\u0026gt;% summarise_all(sum) ##to add columns for lat,long, stat etc, we group ling_data by zip, #and then report the MODE of each of the required columns #group by city, get the first state, most frequent occuring city, and most frequent occuring lat and log #to make it easier will moduralise it:##na.last =NA removes NAs ling_data$newZIP \u0026lt;- substr(as.character(ling_data$ZIP), 1, nchar(ling_data$ZIP) - 2) get_mode \u0026lt;- function(x) { #return(names(sort(table(x, use.NA = \u0026#39;always\u0026#39;), decreasing = T, na.last = T)[1])) #return(which.max(tabulate(x))) ux \u0026lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } temp \u0026lt;- ling_data %\u0026gt;% group_by(newZIP) %\u0026gt;% summarise(state = get_mode(STATE), city = get_mode(CITY), lat = get_mode(as.numeric(lat)), long = get_mode(as.numeric(long))) ##by_zip_ll has all the added columns by_zip_ll \u0026lt;- merge(by_zip, temp[,c(\u0026quot;lat\u0026quot;,\u0026quot;long\u0026quot;, \u0026quot;newZIP\u0026quot;,\u0026quot;state\u0026quot;,\u0026quot;city\u0026quot;)], by.x = \u0026#39;zip\u0026#39;, by.y = \u0026#39;newZIP\u0026#39;, all.x = T) ##adding state info using data(states) by_zip_ll$state \u0026lt;- as.character(by_zip_ll$state) data(state) state_info \u0026lt;- data.frame(stringsAsFactors = F, state.abb, state.region) by_zip_ll \u0026lt;- merge(by_zip_ll, state_info, by.x = \u0026quot;state\u0026quot;, by.y = \u0026quot;state.abb\u0026quot;) ##finally, remove hawaki and alaska by_zip_ll \u0026lt;- by_zip_ll %\u0026gt;% filter(!(state == \u0026#39;AK\u0026#39; | state == \u0026#39;HI\u0026#39;)) just_zip \u0026lt;- by_zip_ll ##let\u0026#39;s change frequenceies to relative frequencies for PCA and KMEANS, DBSCAN ETC. temp \u0026lt;- by_zip_ll[,-c(1,2, 471:747)] temp \u0026lt;- t(apply(temp, 1, function(i) i/sum(i))) ##transpose because R populates by column ##sanity check #rowSums(temp) by_zip_ll[,-c(1,2, 471:747)] \u0026lt;- temp Introduction The study of aggregate linguistic properties over spatial variation is called dialectometry, a sub branch of dialectology: the study of dialects. As language variation is complex, both geographically and dynamically, computational techniques, that can deal with large amounts of granular data, and statistic tehcniques, that can help make inferences from this data, are pivotal for the advancement of dialectometry.\nIn 2003, a dialect survey was condcted as part of an expansion of an initiative started by Professor Bert Vaux at Harvard University. The Dialect Survey uses a series of questions, including rhyming word pairs and vocabulary words, to explore words and sounds in the English language. The survey was conducted to obtain a contemporary view of American English dialectal variation. It started as an online survey, with a final tally of around 47,000 respondents. For this report, we’re interested in the lexical-variant questions, rather than phoenetical variation.\nBy analysing the responses to these questions, we are interested in investigating some geographical structure that might be present in this data. In this report, we’ll explore some dimension reduction methods, and also use some clustering methods to cluster observations into geographically-meaningful groups, using k-means and hierarchical bipartite spectral clustering.\n Dataset The survey dataset contains a set of 122 questions. Each question has around 47,000 responses. For our analyses and clustering, we group the data the first 3 digits of the respondents ZIP code. U.S. ZIP Code Areas (Three-Digit) represents the first three digits of a ZIP Code. The first digit of a five-digit ZIP Code divides the United States into 10 large groups of states numbered from 0 in the Northeast to 9 in the far West.\nWithin these areas, each state is divided into an average of 10 smaller geographical areas, identified by the second and third digits. These digits, in conjunction with the first digit, represent a sectional center facility or a mail processing facility area.\nThere are around ~800 such areas. Each question has a varying degree of possible responses, summarised in ‘answers’ data. Each row represents an individuals reponse, along with their city, state and ZIP, although this was user input so is extremely essy (specially the city). The main dataset, ‘ling_data’ contains this information. In the data cleaning section, I will explain how we sufficied through these challenges.\nData Cleaning The first step was to fix the ling_data. I used the package ‘zipcode’, which has all the unique zipcodes of United States, along with the corresponding city and State. Before merging ling_data with this dataset, I had to add a leading ‘0’ before the 4 digit ZIPs. After merging on the zip code, I was able to remove all the messy entries of ‘cities’ and ‘states’.\n After that, I subsetted the data to our questions of interests, i.e the lexical questions. Then, I changed the ~47,000 x 67 categorical response matrix to a ~47,000 x 468 binary matrix. To illustrate: Question 65 has 6 options. If person A picked option 4, their corresponding entry would become (0,0,0,1,0,0). I also changed the column names to the answer options.\n Then, I removed all respondents who hadn’t answered all the questions, that is, their rows in the binary matrix did not sum to 67. This is to avoid skewing the data.\n Next, I grouped by the 3-digit zip column by adding all the responses and selecting the mode of city, state, latitude and longitude within that zipcode. I removed Alaska and Hawaii from the dataset to make graphical representation easier.\n Finally, I kept two dataframes for analyses, the one described above, and one in which I scale every observation within that zip by total observations in the zip. This is to normalise zips with too many or too few respondents.\n   Exploratory Data Analysis I picked question 105 - What do you call a carbonated beverage? and question 65 - what do you call the insect that glows in the dark because they involve words that people use in common everyday dialect and it’s usually an either-or situation. (CROSSTALK WIDGET)\nTo investigate further, I created an ID column for every unique combination of possible answers for both questions (without ‘other’), and then I removed the ID’s with a frequency fewer than 5,000.\nThere are 6 unique combinations occueing more than 3000 times. When we investigate those:\nWhile combination 19 and 20 seem to dominate the west coast, the rest seem fairly evenly spread over the other regions (combination 9 and 8 seems promiment). There are precisely:\nCombination 19: Use ‘firefly’ and ‘soda’ Combination 20: Use ‘firefly’ and ‘pop’ Combination 9: Use ‘lightening bug’ and ‘pop’ Combination 8: Use ‘lightening bug’ and pop’.    Dimension reduction methods As a first step towards dimesnsion reduction, I used Principal Component Analysis. For this, I centered the data. If not, the geometric interpretation of PCA shows that the first principal component will be close to the vector of means and all subsequent PCs will be orthogonal to it, which will prevent them from approximating any PCs that happen to be close to that first vector. I didn’t however, scale the data, instead decided to scale it by the size of the zipcode.\nA note: It is not a good idea to perform PCA or any other metric-based dimensino reduction on the original data. The challenge with categorical variables is to find a suitable way to represent distances between variable categories and individuals in the factorial space. While PCA can be still be done for binary data, for categorical data,\n Results of PCA When I colour the observations by region, theres seem to some clusters, but because the Screeplot is not explaining a lot of variation in the first 10 dimesnions, I decide to conduct a TNSE and metric Multi Dimensional Scaling.\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction. t-Distributed stochastic neighbor embedding (t-SNE) minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. It is mainly a data exploration and visualization technique.\nMulti Dimensional Scaling (MDS) depends on a distance metric. For this dataset I chose pearson correlation, since I’m more interested in the ‘profile’ of an observation. Multidimensional scaling (MDS) is an established statistical technique that has sometimes been used in language study (see Wheeler (2005)).\nThe results are as follows:\nIn tSNE and MDS we see that there seems to significant clustering according to region of the observation. t-SNE seems to clear the more clear and well-demarcated clusters. In PCA, however, clustering seems weaker.\n  Clustering K- MEANS My first approach was to use k-means to group the clusters. k-means is relatively computationally less expensive and is a good starting point to assess the validity of clusters. it’s useful when we have some sort of a plausible idea of how many clusters exist in the dataset.\nI tried three metrics - Silhouette, Within sum of squares, and gap statistic to arrive at an optimal \\(k\\). ‘WSS’ is usually ambiguous and unreliable.\nBoth the Silhouette Method and Gap Statistic suggest less than 4 clusters. However, when we run a k-means with \\(k\\) = 2,3,4 and 5, we see that k = 4 seems to give the most uniform groups: (CROSSTALK WIDGET)\n Hierarchical Bipartite Spectral Graph Partioning The BiSGP method is based on calculating the singular value decomposition of the input matrix. The hierarchical clustering is obtained by repeatedly clustering the input matrix into two groups. An extensive mathematical explanation as well as an example of the BiSGP method is provided by Wieling and Nerbonne (2010, 2011). Dhillon first introduced this in his 2003 paper: https://www.cs.utexas.edu/users/inderjit/public_papers/kdd_bipartite.pdf\n Importance Within A Cluster Wieling and Nerbonne (2011) proposed a method to measure the importance of a linguistic feature (in our case a specific answer option) in a cluster by combining two measures, representativeness and distinctiveness.\nRepresentativeness of a variant measures how frequently it occurs in the postcode areas in the cluster. For example, if a cluster consists of ten postcode areas and the variant occurs uniquely in six postcode areas, the representativeness is 0.6.\nDistinctiveness of a variant measures how frequently the variant occurs within as opposed to outside the cluster (while taking the relative size of the clusters into account). For example, a distinctiveness of 1 indicates that the variant is not used outside of the cluster.\nFor example, we find that in Cluster 4, the two important questions variants are in Q58, same as Cluster 3. Taking a look at the questinos database tells us that this question is Which of these terms do you prefer for a sale of unwanted items on your porch, in your yard, etc.?\n In cluster 2, one of the most important question is about correct use of Pantyhose are so expensive anymore that I just try to get a good suntan and forget about it.\n Similarly in cluster 1 one of the most important questions is What do you call a public railway system (normally underground)? and *“Would you say ‘Are you coming with?’ as a full sentence, to mean ‘Are you coming with us?’*\n  .\n  Stability of findings to perturbation Since k-means and BiSGP depend on random selection of center points, it influences the stability of conclusions. BiSGP method seemed pretty stable because it gave almost the same top 10 most relevant variants for each time I ran the code with different seeds.\nWe see that the ‘most important’ questions do seem to change we subsample. I find to be logical because of the clustering is being done on the rows and the columns in BiSGP.\nk-means For k-means,I subsampled from the data 100 times, and averaged the ‘center matrix’ and compared it the center matrix to the original data.\nThe centers were off at an average of 3.6 units.\n  Conclusion Reshaping data to make it suitable for analyses is very important. In a data structure like this, many important restructuring decisions, like whether to turn categorical to binary, to scale or not, which distance metric to use, all matter as much as the method of dimesion reduction/ clustering we attempt to do.\nDialectrometry and linguistic data in general has great scope for complex analyses, and can be used not only to ascertain spatial trends but perhaps also population characteristics like gender, age, etc.\n References Bipartite spectral graph partitioning for clustering dialect varieties and detecting their linguistic features - Martijn Weiling, John Nerbonne\n Co-clustering documents and words using Bipartite Spectral Graph Partitioning - Inderjit S. Dhillon\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"29181dac733f35723c710732a52c8436","permalink":"/post/linguistic-survey-data-can-we-detect-regionalities/linguisticsurvey/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/linguistic-survey-data-can-we-detect-regionalities/linguisticsurvey/","section":"post","summary":"This data analysis was undertaken as a part of the Cal’s Stat 215A.\n##Please set path before running: path to where the main data is ##libraries: library(kableExtra) library(dbscan) library(factoextra) library(fastcluster) library(FactoMineR) library(NbClust) library(tidyverse) library(magrittr) library(cluster) library(cowplot) library(NbClust) library(clValid) library(ggfortify) library(clustree) library(dendextend) library(factoextra) library(FactoMineR) library(corrplot) library(GGally) library(knitr) library(kableExtra) library(gplots) library(wesanderson) library(ggplot2) library(tidyverse) library(maps) library(crosstalk) library(readr) library(gridExtra) library(leaflet) library(MASS) library(Rtsne) library(irlba) state_df \u0026lt;- map_data(\u0026quot;state\u0026quot;) my_map_theme \u0026lt;- theme_void() # load the data ling_data \u0026lt;- read.","tags":null,"title":"Linguistic Data Analysis","type":"post"},{"authors":null,"categories":null,"content":" This analysis was understaken as a part of Cal’s Analysis of Time Series class.\nDataset From Kaggle, I downloaded the dataset for rainfall in India from 1901 to 2015, and filtered it to get data only for the city of New Delhi. Then, I made two time series for inital inspection: An annual one, and a monthly one. Since India a tropical country, rainfall is highly seasonal, so it makes sense to retain the monthly data.\nlibrary(data.table) library(forecast) library(ggplot2) library(astsa) rainIndia \u0026lt;- fread(\u0026quot;https://raw.githubusercontent.com/malvikarajeev/misc/master/rainfall%20in%20india%201901-2015.csv\u0026quot;) rainDelhi \u0026lt;- rainIndia[SUBDIVISION == \u0026quot;HARYANA DELHI \u0026amp; CHANDIGARH\u0026quot;] rainDelhi$total \u0026lt;- rowSums(rainDelhi[,c(3:14)]) rain_seasonal \u0026lt;- t(as.matrix(rainDelhi[,c(3:14)])) rain_seasonal \u0026lt;- unlist(as.list(rain_seasonal)) rain_month \u0026lt;- as.data.frame(rain_seasonal) names(rain_month) \u0026lt;- \u0026quot;rain\u0026quot; delhiTs \u0026lt;- rainDelhi[,c(2,20)] head(delhiTs) ## YEAR total ## 1: 1901 390.2 ## 2: 1902 419.7 ## 3: 1903 428.9 ## 4: 1904 527.5 ## 5: 1905 322.8 ## 6: 1906 593.7 Now, to convert the data to a time series object in R.\nSummary Statistics rain \u0026lt;- ts(delhiTs$total, start = c(1901)) rainbymonth \u0026lt;- ts(rain_seasonal, start = c(1901), frequency = 12) plot.ts(rain, main = \u0026quot;Rainfall by Year\u0026quot;) plot.ts(rainbymonth, main = \u0026quot;Rainfall by Month over the Years\u0026quot;) When we view the aggregate annual data, there does seem to be some random fluctuations over time, but they seem consistent. When we view the monthly data over the years, there is clearly a seasonal component. Therefore it becomes a time series with frequency of 12.\n Comparing monthly mean rainfall monthmean \u0026lt;- data.table(1:12) monthmean$mean \u0026lt;- colMeans(rainDelhi[,3:14]) names(monthmean)[1] \u0026lt;- \u0026quot;month\u0026quot; annualmean \u0026lt;- mean(rainbymonth) ggplot() + geom_line(data = monthmean, aes(x = month, y = mean)) + geom_point(data = monthmean, aes(x = month, y = mean), color = \u0026quot;blue\u0026quot;) + geom_hline(yintercept = annualmean, linetype=\u0026quot;dashed\u0026quot;) + labs(title = \u0026quot;Mean Monthly Rainfall\u0026quot;, x = \u0026quot;Month\u0026quot;, y = \u0026quot;Rainfall\u0026quot;) + theme_classic() It is very clear that the months of July and June have a very high amount of rainfall, and so excluding theese two months, the rest of the months seem to have low variance around their mean. Monthly analysis of rainfall indicates that the region has very little or no change in non-monsoon months of January, February, March, November and December.\n#Periodogram spectrum(rainbymonth, log = \u0026quot;no\u0026quot;) spectrum(rainDelhi$ANNUAL) Generally speaking, if a time series appears to be smooth, then the values of the periodogram for low frequencies will be large relative to its other values and we will say that the data set has an excess of low frequency.\nIf a time series has a strong sinusoidal signal for some frequency, then there will be a peak in the periodogram at that frequency. If a time series has a strong nonsinusoidal signal for some frequency, then there will be a peak in the periodogram at that frequency but also peaks at some multiples of that frequency. The first frequency (10 in this case) is called the fundamental frequency and the others called harmonics.   Smoothening library(ggplot2) rain_month$time_period \u0026lt;- seq(from = as.Date(\u0026quot;1/1/1901\u0026quot;, \u0026quot;%d/%m/%Y\u0026quot;), to = as.Date(\u0026quot;31/12/2015\u0026quot;, \u0026quot;%d/%m/%Y\u0026quot;), by = \u0026quot;month\u0026quot;) ##SMOOTHENING: LOESS decomp_2 \u0026lt;- ggplot(rain_month, aes(x = time_period, y = rain)) + geom_line() + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE, span = 0.2, aes(colour = \u0026quot;h=0.2\u0026quot;)) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE, span = 0.4, aes(color = \u0026quot;h=0.4\u0026quot;)) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE, span = 0.6, aes(color = \u0026quot;h=0.6\u0026quot;)) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE, span = 0.8, aes(color = \u0026quot;h=0.8\u0026quot;)) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE, span = 1, aes(color = \u0026quot;h=1\u0026quot;)) + scale_colour_manual(\u0026quot;\u0026quot;, breaks = c(\u0026quot;h=0.2\u0026quot;,\u0026quot;h=0.4\u0026quot;,\u0026quot;h=0.6\u0026quot;,\u0026quot;h=0.8\u0026quot;,\u0026quot;h=1\u0026quot;), values = c(\u0026quot;red\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;blue\u0026quot;,\u0026quot;yellow\u0026quot;,\u0026quot;brown\u0026quot;)) + xlab(\u0026quot; \u0026quot;) + labs(title=\u0026quot;Different LOESS parameters\u0026quot;) + theme_classic() decomp_2 It is clear that LOESS smoothening is giving us a biased curve for all values of the parameter.\n Is the Series Stationary? Making a time series stationary is required to fit a seasonal ARIMA model. A stationary time series in one which the mean and variances level remains near-constant, and the choice of time origin doesn’t change the overall movement of the time series. A time series has a trend, seasonal, and random part. A seasonal part of this time series obviously exists. Now, to determine the trend part.\nreg \u0026lt;- lm(ANNUAL ~ YEAR, rainDelhi) rain_yearly \u0026lt;- ggplot(rainDelhi) + geom_line(aes(x = YEAR, y = ANNUAL)) + geom_abline(intercept = 646, slope = -0.059) + theme_classic() rain_yearly VARIANCE around mean is 20215.02. The slope of the annual time series is -0.5 mm / 10 years, giving about a decrease of aprx 6.8 in the time period from 1901 to 2015.\nI used Kwiatkowski-Phillips-Schmidt-Shin (KPSS) to determine stationarity, and the Mann-Kendall test for monotonic trend detection.\nlibrary(trend) library(tseries) adf.test(rainbymonth) ## ## Augmented Dickey-Fuller Test ## ## data: rainbymonth ## Dickey-Fuller = -9.2452, Lag order = 11, p-value = 0.01 ## alternative hypothesis: stationary kpss.test(rainbymonth, null=\u0026quot;Trend\u0026quot;)$p.value ## [1] 0.1 kpss.test(rainbymonth, null=\u0026quot;Level\u0026quot;)$p.value ## [1] 0.1 #stationarise rainComponents \u0026lt;- decompose(rainbymonth) plot(rainComponents) rain_stat \u0026lt;- rainbymonth - rainComponents$seasonal plot(rain_stat, main = \u0026quot;monthy rain without seasonal component\u0026quot;) ##kendal test for trend result, for annual rainfall and monthly. library(trend) kendall \u0026lt;- mk.test(rainDelhi$ANNUAL) kendall$pvalg ## [1] 0.8961508 kendall2 \u0026lt;- mk.test(rainbymonth) kendall2$pvalg ## [1] 0.4752369 cat(\u0026quot;there there is no monotonic trend in the annual rainfall data\u0026quot;)  ## there there is no monotonic trend in the annual rainfall data After performing the Ljung-Box test for stationarity, I concluded that the monthly rain data is not sufficiently stationary. So I subtracted the seasonal part. After that, the test yields an acceptable p-value.\nboxtest \u0026lt;- Box.test(rainbymonth, lag = 12) boxtest$p.value ## [1] 0 ##stationarise time series by removing seasonal part rain_stat \u0026lt;- rainbymonth - rainComponents$seasonal Box.test(rain_stat, lag = 12, type = \u0026quot;Ljung-Box\u0026quot;) ## ## Box-Ljung test ## ## data: rain_stat ## X-squared = 18.167, df = 12, p-value = 0.1107  Seasonal Differencing The seasonal difference of a time series is the series of changes from one season to the next. For monthly data, in which there are 12 periods in a season, the seasonal difference of Y at period t is \\(Y_t - Y_{t-12}\\).\nboxplot(rainbymonth~cycle(rainbymonth)) acf(rainbymonth) pacf(rainbymonth) Seasonal effect becomes apparent. Even though the mean value of each month apart from the june and july is quite different their variance is small. Hence, we have strong seasonal effect with a cycle of 12 months or less. The time series data should be seasonally differenced, (due to a few spikes in both the ACF and PACF that cut the 95% confidence limits), by order D=1, in order to eliminate seasonality.\nrain_ts \u0026lt;- rainbymonth rainbymonth \u0026lt;- window(rain_ts, end = c(2004,12)) raincheck \u0026lt;- window(rain_ts, start = c(2005,1)) rain_diff \u0026lt;- diff(rainbymonth,36, difference = 1) rain_stat_diff \u0026lt;- diff(rain_stat, 36, difference =1) Acf(rain_diff, lag.max = 50, main = \u0026quot;ACF after seasonal differencing\u0026quot;) Pacf(rain_diff, lag.max = 50, main = \u0026quot;PACF after seasonal differencing\u0026quot;) On inspection, it seems like a SARIMA model with seasonal parameters:\nAR = 1 or 0 MA = 1 or 0  and non seasonal parameters:\nAR = 0 MA = 1 or 2  So we run this model , and get the auto.arima model and compare results. The auto.arima model suggested \\((0,0,0,2,1,1)_{12}\\).\n Model Fit library(astsa) model_1 \u0026lt;- sarima(rainbymonth, 1,0,1,1,1,1,12) ## initial value 4.066829 ## iter 2 value 3.864028 ## iter 3 value 3.812254 ## iter 4 value 3.761534 ## iter 5 value 3.741852 ## iter 6 value 3.738756 ## iter 7 value 3.737280 ## iter 8 value 3.734718 ## iter 9 value 3.734155 ## iter 10 value 3.733970 ## iter 11 value 3.733952 ## iter 12 value 3.733947 ## iter 13 value 3.733946 ## iter 14 value 3.733946 ## iter 15 value 3.733945 ## iter 16 value 3.733945 ## iter 17 value 3.733945 ## iter 18 value 3.733940 ## iter 19 value 3.733934 ## iter 20 value 3.733923 ## iter 21 value 3.733916 ## iter 22 value 3.733912 ## iter 23 value 3.733912 ## iter 23 value 3.733912 ## final value 3.733912 ## converged ## initial value 3.732315 ## iter 2 value 3.731700 ## iter 3 value 3.730950 ## iter 4 value 3.730580 ## iter 5 value 3.730509 ## iter 6 value 3.730496 ## iter 7 value 3.730493 ## iter 8 value 3.730493 ## iter 9 value 3.730493 ## iter 10 value 3.730492 ## iter 11 value 3.730489 ## iter 12 value 3.730482 ## iter 13 value 3.730479 ## iter 14 value 3.730477 ## iter 15 value 3.730477 ## iter 16 value 3.730476 ## iter 17 value 3.730476 ## iter 18 value 3.730475 ## iter 19 value 3.730473 ## iter 20 value 3.730469 ## iter 21 value 3.730459 ## iter 22 value 3.730443 ## iter 23 value 3.730428 ## iter 24 value 3.730425 ## iter 25 value 3.730423 ## iter 26 value 3.730422 ## iter 27 value 3.730422 ## iter 27 value 3.730422 ## iter 27 value 3.730422 ## final value 3.730422 ## converged model_2 \u0026lt;- sarima(rainbymonth, 0,0,0,2,1,0,12) ## initial value 4.070425 ## iter 2 value 3.927099 ## iter 3 value 3.874484 ## iter 4 value 3.871906 ## iter 5 value 3.871527 ## iter 6 value 3.871527 ## iter 6 value 3.871527 ## iter 6 value 3.871527 ## final value 3.871527 ## converged ## initial value 3.866630 ## iter 2 value 3.866613 ## iter 3 value 3.866612 ## iter 3 value 3.866612 ## iter 3 value 3.866612 ## final value 3.866612 ## converged model_1 ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 sar1 sma1 constant ## -0.1324 0.1768 -0.0272 -0.9758 0.0023 ## s.e. 0.8471 0.8421 0.0296 0.0111 0.0041 ## ## sigma^2 estimated as 1687: log likelihood = -6364.61, aic = 12741.22 ## ## $degrees_of_freedom ## [1] 1231 ## ## $ttable ## Estimate SE t.value p.value ## ar1 -0.1324 0.8471 -0.1563 0.8758 ## ma1 0.1768 0.8421 0.2100 0.8337 ## sar1 -0.0272 0.0296 -0.9198 0.3579 ## sma1 -0.9758 0.0111 -88.1313 0.0000 ## constant 0.0023 0.0041 0.5477 0.5840 ## ## $AIC ## [1] 10.2175 ## ## $AICc ## [1] 10.21754 ## ## $BIC ## [1] 10.24213 model_2 ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## sar1 sar2 constant ## -0.6578 -0.3339 0.0064 ## s.e. 0.0269 0.0270 0.0571 ## ## sigma^2 estimated as 2272: log likelihood = -6532.94, aic = 13073.88 ## ## $degrees_of_freedom ## [1] 1233 ## ## $ttable ## Estimate SE t.value p.value ## sar1 -0.6578 0.0269 -24.4354 0.0000 ## sar2 -0.3339 0.0270 -12.3630 0.0000 ## constant 0.0064 0.0571 0.1119 0.9109 ## ## $AIC ## [1] 10.48427 ## ## $AICc ## [1] 10.48428 ## ## $BIC ## [1] 10.50069 auto.arima(rainbymonth, trace=TRUE, seasonal = TRUE) ## ## Fitting models using approximations to speed things up... ## ## ARIMA(2,0,2)(1,1,1)[12] with drift : Inf ## ARIMA(0,0,0)(0,1,0)[12] with drift : 13457.89 ## ARIMA(1,0,0)(1,1,0)[12] with drift : 13125.87 ## ARIMA(0,0,1)(0,1,1)[12] with drift : Inf ## ARIMA(0,0,0)(0,1,0)[12] : 13455.89 ## ARIMA(1,0,0)(0,1,0)[12] with drift : 13459.85 ## ARIMA(1,0,0)(2,1,0)[12] with drift : 12991.15 ## ARIMA(1,0,0)(2,1,1)[12] with drift : Inf ## ARIMA(1,0,0)(1,1,1)[12] with drift : Inf ## ARIMA(0,0,0)(2,1,0)[12] with drift : 12988.55 ## ARIMA(0,0,0)(1,1,0)[12] with drift : 13122.86 ## ARIMA(0,0,0)(2,1,1)[12] with drift : Inf ## ARIMA(0,0,0)(1,1,1)[12] with drift : Inf ## ARIMA(0,0,1)(2,1,0)[12] with drift : 12990.16 ## ARIMA(1,0,1)(2,1,0)[12] with drift : 12993.16 ## ARIMA(0,0,0)(2,1,0)[12] : 12986.55 ## ARIMA(0,0,0)(1,1,0)[12] : 13120.86 ## ARIMA(0,0,0)(2,1,1)[12] : Inf ## ARIMA(0,0,0)(1,1,1)[12] : Inf ## ARIMA(1,0,0)(2,1,0)[12] : 12989.14 ## ARIMA(0,0,1)(2,1,0)[12] : 12988.15 ## ARIMA(1,0,1)(2,1,0)[12] : 12991.15 ## ## Now re-fitting the best model(s) without approximations... ## ## ARIMA(0,0,0)(2,1,0)[12] : 13071.91 ## ## Best model: ARIMA(0,0,0)(2,1,0)[12] ## Series: rainbymonth ## ARIMA(0,0,0)(2,1,0)[12] ## ## Coefficients: ## sar1 sar2 ## -0.6578 -0.3339 ## s.e. 0.0269 0.0270 ## ## sigma^2 estimated as 2275: log likelihood=-6532.95 ## AIC=13071.89 AICc=13071.91 BIC=13087.25 ##predicting prediction_1 \u0026lt;- sarima.for(rainbymonth, 132,1,0,1,1,1,1,12) prediction_2 \u0026lt;- sarima.for(rainbymonth, 132,0,0,0,2,1,0,12) library(forecast) #fit \u0026lt;- auto.arima(rainbymonth,max.p = 5,max.q = 5,max.P = 5,max.Q = 5,max.d = 3,seasonal = TRUE,ic = \u0026#39;aicc\u0026#39;) #plot(forecast(fit,h=20)) #hist(fit$residuals) A fitted model should be subjected to diagnostic checking with a view to ascertaining its goodness-of-fit to the data. This is done by analysing its residuals. An adequate model should have uncorrelated residuals. This is the minimal condition. The optimal condition is that the residuals should follow a Gaussian distribution with mean zero. The residuals resemble white noise which is a good indication.\nNow I plot the actual values predictied from 1901 - 2004 for 2005-2015 against the actual data of 2005-2015.\nformat \u0026lt;- function(x) { temp \u0026lt;- unlist(as.list(t(as.matrix(x)))) temp \u0026lt;- as.data.table(temp) temp$time \u0026lt;- 1:nrow(temp) return(temp) } predict_1 \u0026lt;-format(prediction_1$pred) predict_2 \u0026lt;- format(prediction_2$pred) raincheck \u0026lt;- format(raincheck) ##prediction frame: predict_1$se \u0026lt;- unlist(as.list(t(as.matrix(prediction_1$pred)))) predict_1$lower \u0026lt;- predict_1$temp - 1.96*predict_1$se predict_1$upper \u0026lt;- predict_1$temp + 1.96*predict_1$se ggplot() + geom_line(data = predict_1,aes(x=time,y = temp, colour = \u0026quot;model1\u0026quot;)) + geom_line(data = predict_2,aes(x=time,y = temp, colour = \u0026quot;model2\u0026quot;)) + geom_line(data = raincheck,aes(x=time,y = temp, colour = \u0026quot;truth\u0026quot;)) + scale_colour_manual(\u0026quot;\u0026quot;, breaks = c(\u0026quot;model1\u0026quot;,\u0026quot;model2\u0026quot;,\u0026quot;truth\u0026quot;), values = c(\u0026quot;red\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;blue\u0026quot;)) + geom_ribbon(data=predict_1,aes(x = time,ymin=lower,ymax=upper),alpha=0.3) + labs(title=\u0026quot;Prediction v Truth with 95% confidence bands\u0026quot;) + theme_classic() The predictions seem not too far from the truth, and within the confidence intervals. The AIC’s for the SARIMA models are almost identical.\n Conclusion: Rainfall in Delhi is highly seasonal, with peak during July and June.\n There is no ascertainable overall monotonic trend in the data.\n Series can be made sufficiently stationary by subtracting the seasonal component, and by differencing (degree 1) over a frequency of 12 months.\n SARIMA models are adequate in predicting weather forecasts.\n    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f2e7d71844f3d05d9b2bef27f122c122","permalink":"/post/rainfall-analysis/timeseriesproject/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/rainfall-analysis/timeseriesproject/","section":"post","summary":"This analysis was understaken as a part of Cal’s Analysis of Time Series class.\nDataset From Kaggle, I downloaded the dataset for rainfall in India from 1901 to 2015, and filtered it to get data only for the city of New Delhi. Then, I made two time series for inital inspection: An annual one, and a monthly one. Since India a tropical country, rainfall is highly seasonal, so it makes sense to retain the monthly data.","tags":null,"title":"Rainfall in New Delhi, India: 1901 - 2015","type":"post"},{"authors":null,"categories":null,"content":" So a while ago my friend who just started using RStudio and is a bit overexcited about working with data told me about getting all your spotify data is totally doable, all you gotta do is email spotify. Follow the link here for more details: https://support.spotify.com/ca-en/article/data-rights-and-privacy-settings/\nSpotify will email your data in a zip format in JSON files. I use the jsonlite package in R to read the data in.\nurlspot = \u0026quot;https://raw.githubusercontent.com/malvikarajeev/spotifyAnalysis/master/\u0026quot; spot0 = jsonlite::read_json(paste(urlspot,\u0026quot;StreamingHistory0.json\u0026quot;, sep = \u0026quot;\u0026quot;), simplifyVector = T) spot1 = jsonlite::read_json(paste(urlspot,\u0026quot;StreamingHistory1.json\u0026quot;, sep = \u0026quot;\u0026quot;), simplifyVector = T) spot = rbind(spot0, spot1) The data is pretty straightforward: the time the track ended streaming, artist and track name, and the milliseconds it was listened to. I’ll use shiny to visualise my streaming trends.\nusing lubridate to get end times spot$end_time = as.POSIXct(strptime(spot$endTime, \u0026quot;%Y-%m-%d %H:%M\u0026quot;)) spot$date = date(spot$end_time) spot$month = month(spot$date, label = T) customm = function(date){ temp = strsplit(date, \u0026#39; \u0026#39;) %\u0026gt;% unlist temp2 = temp[2] return(temp2) } spot$only_time = parse_time(sapply(spot$endTime, customm)) my_seconds \u0026lt;- period_to_seconds(hms(spot$only_time)) myIntervals \u0026lt;- c(\u0026quot;0 AM - 6 AM\u0026quot;, \u0026quot;6 AM - 12 PM\u0026quot;, \u0026quot;12 PM - 6 PM\u0026quot;, \u0026quot;6 PM - 0 AM\u0026quot;) spot$interval \u0026lt;- myIntervals[findInterval(my_seconds, c(0, 6, 12, 18, 24) * 3600)] ##I want to group by interval, trackName, sum up the milliseconds, and get highest milisecond for each interval arrranged by trackname interval_artist = spot %\u0026gt;% group_by(interval, trackName) %\u0026gt;% summarise(s = sum(msPlayed)) %\u0026gt;% arrange(-s) %\u0026gt;% top_n(20, s) For shiny documents/chunks, make sure cache = FALSE. Markdown can’t cache shiny stuff since the reactive function already does that.\nShiny can be used to create some pretty interactive visualisations. I wanted to see what kind of music I listen to monthly, and what times. A simple if-else clause in your ggplot can simplify visualisation according to user specification.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f41bb31fe4db9761c7f5d0996141aaa3","permalink":"/post/spotifyanalysis/spotifyanalysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/spotifyanalysis/spotifyanalysis/","section":"post","summary":"So a while ago my friend who just started using RStudio and is a bit overexcited about working with data told me about getting all your spotify data is totally doable, all you gotta do is email spotify. Follow the link here for more details: https://support.spotify.com/ca-en/article/data-rights-and-privacy-settings/\nSpotify will email your data in a zip format in JSON files. I use the jsonlite package in R to read the data in.","tags":null,"title":"Spotify!","type":"post"},{"authors":null,"categories":null,"content":" requesting data from uber The purpose of this exercise is to visualise how I use my uber data. Uber records are pretty useful - just ask uber to email you your data: https://help.uber.com/driving-and-delivering/article/request-your-personal-uber-data?nodeId=fbf08e68-65ba-456b-9bc6-1369eb9d2c44\nI’m removing remore 2020 and 2015 (it contains only one day) for now becuase it might skew the data.\n reading in the data myrides = read.csv(\u0026quot;https://raw.githubusercontent.com/malvikarajeev/uberAnalysis/master/trips_data.csv\u0026quot;) head(myrides) ## City Product.Type Trip.or.Order.Status ## 1 Los Angeles UberX COMPLETED ## 2 Los Angeles UberX COMPLETED ## 3 Los Angeles UberX COMPLETED ## 4 Los Angeles UberX COMPLETED ## 5 Los Angeles UberX COMPLETED ## 6 Los Angeles UberX DRIVER_CANCELED ## Request.Time Begin.Trip.Time ## 1 2020-02-17 04:43:38 +0000 UTC 2020-02-17 04:52:00 +0000 UTC ## 2 2020-02-17 01:17:06 +0000 UTC 2020-02-17 01:19:19 +0000 UTC ## 3 2020-02-16 20:29:34 +0000 UTC 2020-02-16 20:35:00 +0000 UTC ## 4 2020-02-16 18:45:42 +0000 UTC 2020-02-16 18:51:31 +0000 UTC ## 5 2020-02-16 00:17:16 +0000 UTC 2020-02-16 00:23:12 +0000 UTC ## 6 2020-02-15 23:36:22 +0000 UTC 1970-01-01 00:00:00 +0000 UTC ## Begin.Trip.Lat Begin.Trip.Lng ## 1 34.08361 -118.3521 ## 2 33.99775 -118.4748 ## 3 33.96174 -118.3673 ## 4 33.98221 -118.4594 ## 5 34.01283 -118.4966 ## 6 34.01028 -118.4934 ## Begin.Trip.Address ## 1 7422 Melrose Ave, Los Angeles, CA 90046, US ## 2 423 Rose Ave, Venice, CA 90291, US ## 3 621 W Manchester Blvd, Inglewood, CA 90301, US ## 4 4100 Admiralty Way, Marina del Rey, CA 90292, US ## 5 111 Broadway, Santa Monica, CA 90401, US ## 6 ## Dropoff.Time Dropoff.Lat Dropoff.Lng ## 1 2020-02-17 05:01:35 +0000 UTC 34.09819 -118.3077 ## 2 2020-02-17 01:29:59 +0000 UTC 33.98216 -118.4595 ## 3 2020-02-16 20:55:09 +0000 UTC 33.97935 -118.4664 ## 4 2020-02-16 19:07:13 +0000 UTC 33.96225 -118.3671 ## 5 2020-02-16 00:46:38 +0000 UTC 33.98220 -118.4595 ## 6 1970-01-01 00:00:00 +0000 UTC 34.01351 -118.4972 ## Dropoff.Address ## 1 5419 W Sunset Blvd, Los Angeles, CA 90027, US ## 2 4100 Admiralty Way, Marina del Rey, CA 90292, US ## 3 Venice Beach Pier Public Parkingl Lot, Unnamed Road, Marina Del Rey, CA 90292, United States ## 4 621 W Manchester Blvd, Inglewood, CA 90301, US ## 5 4100 Admiralty Way, Marina del Rey, CA 90292, US ## 6 4100 Admiralty Way, Marina del Rey, CA 90292, US ## Distance..miles. Fare.Amount Fare.Currency ## 1 3.56 9.14 USD ## 2 2.21 7.43 USD ## 3 7.27 11.65 USD ## 4 7.24 10.63 USD ## 5 3.81 10.30 USD ## 6 0.00 5.00 USD myrides$completed = ifelse(myrides$Trip.or.Order.Status == \u0026#39;COMPLETED\u0026#39;, T, F) ##basic eda myrides$time_started = as.POSIXct(strptime(myrides$Begin.Trip.Time, \u0026quot;%Y-%m-%d %H:%M:%S\u0026quot;)) myrides$year = year(myrides$time_started) ##remove 1970 and not completed myrides = myrides %\u0026gt;% filter(!(year == 1970 | year == 2020 | year == 2015)) myrides = myrides %\u0026gt;% filter(Product.Type != \u0026#39;UberEATS Marketplace\u0026#39;) myrides = myrides %\u0026gt;% filter(completed == T) myrides$month_year = format(as.Date(myrides$Begin.Trip.Time), \u0026quot;%Y-%m\u0026quot;) ggplot(myrides, aes(x = month_year)) + geom_bar(aes(fill = as.factor(year))) + scale_fill_brewer(palette=\u0026quot;Set1\u0026quot;) + theme_tufte() + theme(axis.text.x = element_blank()) + labs(y = \u0026#39;Frequency of Rides\u0026#39;, x = \u0026#39;Time Period\u0026#39;) + scale_fill_discrete(name = \u0026quot;Year\u0026quot;) ## Scale for \u0026#39;fill\u0026#39; is already present. Adding another scale for \u0026#39;fill\u0026#39;, ## which will replace the existing scale. Seems like on an avergae I took about 10-20 rides a month, seemingly growing with every year. there seems to be a coherent pattern in that number of rides increase monotically as we move from January to February (except for in 2018). The month of September-November seems generally low.\nNow, I moved from New Delhi, India, to Berkeley, California, in the month of August, 2018. Can we see this move reflect different patterns?\n average trip time. myrides$time_ended = as.POSIXct(strptime(myrides$Dropoff.Time, \u0026quot;%Y-%m-%d %H:%M:%S\u0026quot;)) myrides$duration_mins = myrides$time_ended - myrides$time_started myrides$duration_mins = as.integer(myrides$duration_mins) ggplot(myrides, aes(y = duration_mins, x = month_year)) + geom_boxplot() + theme_tufte() + theme(axis.text.x = element_blank()) + labs(y = \u0026#39;Distribution of Rides\u0026#39;, x = \u0026#39;Time Period\u0026#39;) + scale_fill_discrete(name = \u0026quot;Year\u0026quot;) The average time of my rides is decreasing: perhaps it makes sense, the traffic in New Delhi is insane comapred to the traffic in Berkeley.\n fare habits I wanted to group by year, and get the cumulative fare for each year by month. In the pursuit of this, I found a function called ave\nfare_wise = function(currency){ fares = myrides %\u0026gt;% filter(Fare.Currency == currency) %\u0026gt;% group_by(year, month_year) %\u0026gt;% summarise(monthly_fare = sum(Fare.Amount, na.rm = T)) fares$cumulative_fare = ave(fares$monthly_fare, fares$year, FUN = cumsum) return(fares) } inr = fare_wise(\u0026#39;INR\u0026#39;) ##Adding year ## ggplot(inr) + geom_point(aes(y = cumulative_fare, x = month_year, color = factor(year))) + theme_tufte() + transition_states(year, wrap = T)   # labs(title = \u0026quot;Year: {frame_time}\u0026quot;) + # view_follow(fixed_x = T) # anim_save(\u0026quot;inr.gif\u0026quot;, animation = gg, path = \u0026quot;/figures\u0026quot;) What the hell was I doing in 2017… damn.\ntemp = myrides %\u0026gt;% filter(Begin.Trip.Lat * Begin.Trip.Lng != Dropoff.Lat*Dropoff.Lng) %\u0026gt;% filter(Fare.Currency == \u0026#39;USD\u0026#39;) usa_map = map_data(\u0026quot;county\u0026quot;) ca_df \u0026lt;- usa_map %\u0026gt;% filter(region == \u0026#39;california\u0026#39;) # ggplot() + # geom_polygon(data = ca_df, aes(x=long, y = lat)) + # coord_fixed(1.3) + # geom_curve(data=temp, # aes(x=Begin.Trip.Lng, y=Begin.Trip.Lat, xend=Dropoff.Lng, yend=Dropoff.Lat), # col = \u0026quot;#b29e7d\u0026quot;, size = 1, curvature = .2) + # geom_point(data=temp, # aes(x=Dropoff.Lng, y=Dropoff.Lat), # colour=\u0026quot;blue\u0026quot;, # size=1.5) + # geom_point(data=temp, # aes(x=Begin.Trip.Lng, y=Begin.Trip.Lat), # colour=\u0026quot;blue\u0026quot;) + # theme(axis.line=element_blank(), # axis.text.x=element_blank(), # axis.text.y=element_blank(), # axis.title.x=element_blank(), # axis.title.y=element_blank(), # axis.ticks=element_blank(), # plot.title=element_text(hjust=0.5, size=12)) library(shiny) library(ggmap) library(ggplot2) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;My Uber Rides\u0026quot;), sidebarLayout( # sidebarPanel( # radioButtons(\u0026quot;radio\u0026quot;, label = h4(\u0026quot;Choose currency\u0026quot;), # choices = list(\u0026quot;USD\u0026quot; = \u0026#39;USD\u0026#39;, \u0026quot;INR\u0026quot; = \u0026#39;INR\u0026#39;)), # radioButtons(\u0026quot;interval\u0026quot;, label = h4(\u0026quot;show time of day?\u0026quot;), # choices = list(\u0026quot;Yes\u0026quot; = TRUE, \u0026quot;No\u0026quot; = FALSE)), selectInput(\u0026quot;month_year\u0026quot;, label = \u0026quot;Choose Month and Year\u0026quot;, choices = unique(temp$month_year)), sliderInput(\u0026quot;duration_ride\u0026quot;, \u0026quot;Duration of Rides\u0026quot;, min = 1, max = max(temp$duration), value = c(1,10)) ), mainPanel(plotOutput(outputId = \u0026quot;my_map\u0026quot;) ) ) ) #load() server \u0026lt;- function(input, output) { outputR = reactive({ req(input$duration_ride) req(input$month_year) temp2 = temp%\u0026gt;% filter(duration_mins \u0026lt;= input$duration_ride[2] \u0026amp; duration_mins \u0026gt;= input$duration_ride[1]) %\u0026gt;% filter(month_year == input$month_year) usa_map = map_data(\u0026quot;county\u0026quot;) ca_df \u0026lt;- usa_map %\u0026gt;% filter(region == \u0026#39;california\u0026#39;) long = mean(temp2$Dropoff.Lng, na.rm = T) latt = mean(temp2$Dropoff.Lat, na.rm = T) g = ggmap(get_googlemap(c(long, latt), zoom = 15 , scale = 2, maptype =\u0026#39;roadmap\u0026#39;, color = \u0026#39;color\u0026#39;, archiving = T)) + geom_segment(data=temp2, aes(x=Begin.Trip.Lng, y=Begin.Trip.Lat, xend=Dropoff.Lng, yend=Dropoff.Lat), col = \u0026quot;black\u0026quot;, size = 0.3, arrow = arrow()) + geom_point(data=temp2, aes(x=Dropoff.Lng, y=Dropoff.Lat, colour=\u0026quot;red\u0026quot;), alpha = 0.5) + geom_point(data=temp2, aes(x=Begin.Trip.Lng, y=Begin.Trip.Lat, colour=\u0026quot;blue\u0026quot;), alpha = 0.5) + scale_color_identity( breaks = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), labels = c(\u0026quot;Drop off Point\u0026quot;, \u0026quot;Pick up point\u0026quot;), guide = \u0026quot;legend\u0026quot;) g }) output$my_map= renderPlot({outputR()}) } shinyApp(ui = ui, server = server)   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bd68445285e7d9b14be9c96d287d29e3","permalink":"/post/uberanalysis/uberanalysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/uberanalysis/uberanalysis/","section":"post","summary":"requesting data from uber The purpose of this exercise is to visualise how I use my uber data. Uber records are pretty useful - just ask uber to email you your data: https://help.uber.com/driving-and-delivering/article/request-your-personal-uber-data?nodeId=fbf08e68-65ba-456b-9bc6-1369eb9d2c44\nI’m removing remore 2020 and 2015 (it contains only one day) for now becuase it might skew the data.\n reading in the data myrides = read.csv(\u0026quot;https://raw.githubusercontent.com/malvikarajeev/uberAnalysis/master/trips_data.csv\u0026quot;) head(myrides) ## City Product.Type Trip.or.Order.Status ## 1 Los Angeles UberX COMPLETED ## 2 Los Angeles UberX COMPLETED ## 3 Los Angeles UberX COMPLETED ## 4 Los Angeles UberX COMPLETED ## 5 Los Angeles UberX COMPLETED ## 6 Los Angeles UberX DRIVER_CANCELED ## Request.","tags":null,"title":"uber data","type":"post"}]